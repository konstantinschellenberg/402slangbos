df = do.call("rbind", dataSet)
df_all = rbind(df_all, df)
# ----------------------------------------------------------------------
# rename inner lists (of the categories)
names(out) = c(seq(1:nrow(categorymap)))
# append to master-list (outest)
outest = append(outest, list(out))
}
names(outest) = unique(train_data[[response_col]])
saveRDS(df_all, paste0(rds_path, "learning_input", outfile, ".rds"))
saveRDS(outest, paste0(rds_path, "gt_list", outfile, ".rds"))
}
s1vv = s1vv[1:10]
names(s1vv)
s1vv = s1vv[[1]]
plot(s1vv)
source("import.R")
s1vv = s1vv[[1:10]]
source("import.R")
s1vv_re = s1vv[[1:10]]
View(s1vv_re)
1:10
seq(1,10
)
s1vv[[c(seq(1,10))]]
s1vv_re = s1vv[[c(seq(1,10))]]
s1vv_re
names(s1vv_re)
[[seq(1, 10)]]
seq(1, 10)
list_rasters = list(s1vv, s1vh)
list_rasters
View(list_rasters)
for i in list_rasters{
i = i[[seq(1, 3)]]
gt_from_raster(train_data = gt, response_col = "Name", raster = i, outfile = chr(i))
}
gt_from_raster(raster = s1vv_re, ooutfile = "test")
gt_from_raster(raster = s1vv_re, outfile = "test")
View(s1vv_re)
s1vv_re = s1vv[[seq(1,3)]]
View(s1vv_re)
View(s1vv)
View(s1vv_re)
gt_from_raster(raster = s1vv_re, outfile = "test")
gt_from_raster = function(train_data = gt,
response_col = "Name",
raster = s1vh,
outfile = "s1vh"){
# credits to http://amsantac.co/blog/en/2015/11/28/classification-r.html
# https://gist.github.com/amsantac/5183c0c71a8dcbc27a4f
df_all = data.frame(matrix(vector(), nrow = 0, ncol = length(names(raster)) + 1))
outest = list()
for (i in 1:length(unique(train_data[[response_col]]))){
# get class
category = unique(train_data[[response_col]])[i]
print(category)
# returns sp polygon with class i
categorymap = train_data[train_data[[response_col]] == category,]
# extract pixel information
dataSet = raster::extract(raster, categorymap)
out = list()
for (a in 1:length(dataSet)){
print("inner", a)
t = as.data.frame(dataSet[[a]]) %>%
t() %>%
as.data.frame() %>%
mutate(date = colnames(dataSet[[1]])) %>%
pivot_longer(-date, names_to = "names", values_to = "values") %>%
group_by(date) %>%
summarise(mean = mean(values), # here can be put more stats information retrieved from the polygons
median = median(values),
sd = sd(values),
"lower_sd" = mean(values) - sd(values),
"upper_sd" = mean(values) + sd(values),
count = n())
out = append(out, list(as.data.frame(t)))
}
# making dataset for machine learning-----------------------------------
dataSet = dataSet[!unlist(lapply(dataSet, is.null))]
dataSet = lapply(dataSet, function(x){cbind(x, class = as.numeric(rep(category, nrow(x))))})
df = do.call("rbind", dataSet)
df_all = rbind(df_all, df)
# ----------------------------------------------------------------------
# rename inner lists (of the categories)
names(out) = c(seq(1:nrow(categorymap)))
# append to master-list (outest)
outest = append(outest, list(out))
}
names(outest) = unique(train_data[[response_col]])
saveRDS(df_all, paste0(rds_path, "learning_input", outfile, ".rds"))
saveRDS(outest, paste0(rds_path, "gt_list", outfile, ".rds"))
}
gt_from_raster = function(train_data = gt,
response_col = "Name",
raster = s1vh,
outfile = "s1vh"){
# credits to http://amsantac.co/blog/en/2015/11/28/classification-r.html
# https://gist.github.com/amsantac/5183c0c71a8dcbc27a4f
df_all = data.frame(matrix(vector(), nrow = 0, ncol = length(names(raster)) + 1))
outest = list()
for (i in 1:length(unique(train_data[[response_col]]))){
# get class
category = unique(train_data[[response_col]])[i]
print(category)
# returns sp polygon with class i
categorymap = train_data[train_data[[response_col]] == category,]
# extract pixel information
dataSet = raster::extract(raster, categorymap)
out = list()
for (a in 1:length(dataSet)){
print("inner", a)
t = as.data.frame(dataSet[[a]]) %>%
t() %>%
as.data.frame() %>%
mutate(date = colnames(dataSet[[1]])) %>%
pivot_longer(-date, names_to = "names", values_to = "values") %>%
group_by(date) %>%
summarise(mean = mean(values), # here can be put more stats information retrieved from the polygons
median = median(values),
sd = sd(values),
"lower_sd" = mean(values) - sd(values),
"upper_sd" = mean(values) + sd(values),
count = n())
out = append(out, list(as.data.frame(t)))
}
# making dataset for machine learning-----------------------------------
dataSet = dataSet[!unlist(lapply(dataSet, is.null))]
dataSet = lapply(dataSet, function(x){cbind(x, class = as.numeric(rep(category, nrow(x))))})
df = do.call("rbind", dataSet)
df_all = rbind(df_all, df)
# ----------------------------------------------------------------------
# rename inner lists (of the categories)
names(out) = c(seq(1:nrow(categorymap)))
# append to master-list (outest)
outest = append(outest, list(out))
}
names(outest) = unique(train_data[[response_col]])
saveRDS(df_all, paste0(rds_path, "learning_input", outfile, ".rds"))
saveRDS(outest, paste0(rds_path, "gt_list", outfile, ".rds"))
}
s1vv_re = s1vv[[seq(1,3)]]
gt_from_raster(raster = s1vv_re, outfile = "test")
readRDS(paste0(rds_path, "learning_input", test, ".rds")
readRDS(paste0(rds_path, "learning_input", test, ".rds"))
rds_path
readRDS(paste0(rds_path, "learning_inputtest.rds"))
leaning_input = readRDS(paste0(rds_path, "learning_inputtest.rds"))
leaning_input
gt_list = readRDS(paste0(rds_path, "gt_listtest.rds"))
gt_list
gt_list$1
gt_list$12
gt_list$"12"
system.time(gt_from_raster(raster = s1vh, outfile = "_test"))
######################################################################
# Data Preparation
######################################################################
rds_path
library(mlr)
library(data.table)
library(parallelMap)
library(raster)
# subset for test data
s1vv_re = s1vv[[seq(1,3)]]
# time consuming task!
system.time(gt_from_raster(raster = s1vv_re, outfile = "_test"))
learning_input = readRDS(paste0(rds_path, "learning_input_VH.rds"))
learning_input
names(learning_input)
learning_input_test = readRDS(paste0(rds_path, "learning_input_test.rds"))
View(learning_input_test)
s1vv_re = i
chr(i)
toString(i)
i
i = s1vv_re
toString(i)
for i in list_rasters{
i = i[[seq(1, 3)]]
library(mlr)
library(data.table)
library(parallelMap)
library(raster)
source("import.R")
# Input of stack, which is containing training and reference data
data_input = readRDS(paste0(rds_path, "learning_input_test.rds"))
View(data_input)
data.table::data.table(data_input)
data_input
# Remove NoData values
data <- na.omit(data_input)
# Input of stack, which is containing training and reference data
data_input = readRDS(paste0(rds_path, "learning_input_test.rds"))
data_input = na.omit(data_input)
classif.task <- makeClassifTask(
id = "slangbos", data = data, target = "class"
)
class(data_input$class)
as_factor(data_input$class)
as.factor(data_input$class)
data_input$class = as_factor(data_input$class)
data_input$class
class(data_input$class)
classif.task <- makeClassifTask(
id = "slangbos", data = data, target = "class"
)
data_input$class = as_factor(data_input$class) # assign class as factor, not numeric
classif.task <- makeClassifTask(
id = "slangbos", data = data, target = "class"
)
classif.task <- makeClassifTask(
id = "slangbos", data = data_input, target = "class"
)
classif.task
# Create Learner
# Classification tree, set it up for predicting probabilities
classif.lrn = makeLearner("classif.randomForest", predict.type = "response", fix.factors.prediction = TRUE)
classif.lrn
# show default parameters
getParamSet(classif.lrn)
# show which parameters are tunable
filterParams(getParamSet(classif.lrn), tunable = TRUE)
mlr
vignette("mlr")
# Defining parameter set
ps <- makeParamSet(
makeIntegerParam("mtry", lower = 1, upper = 4),
makeDiscreteParam("num.trees", values = c(10,50,100,300,700))
# for random selection of num.trees use: makeIntegerParam("num.trees", lower = 10, upper = 700)
)
ps
# Define the inner reampling iterations
ctrl <- makeTuneControlGrid() # for random selection of parameters use: ctrl <- makeTuneControlRandom(maxit = 100)
makeResampleDesc()
?makeResampleDesc()
inner <- makeResampleDesc("SpCV", iters = 5)
inner
parallelStart(mode = "socket", level = "mlr.tuneParams", cpus = 5)
# Tuning the Random Forest
tune_rf <- tuneParams(classif.lrn,
task = classif.task, resampling = inner, par.set = ps,
control = ctrl, show.info = TRUE, measures = setAggregation(rmse, test.mean)
)
?Task
s1vv_re
# subset for test data
s1vv_re = s1vv[[seq(1,3)]]
# subset for test data
raster_test = s1vv[[seq(1,3)]]
?raster::extrace
?raster::extract
# Create data frame
data <- as.data.frame(data_input, xy = TRUE)
data
as.data.frame()
?as.data.frame()
?as.data.frame
train_data = gt
response_col = "Name"
raster = s1vh
raster = s1vv_re
df_all = data.frame(matrix(vector(), nrow = 0, ncol = length(names(raster)) + 1))
outest = list()
# get class
category = unique(train_data[[response_col]])[i]
category
# returns sp polygon with class i
categorymap = train_data[train_data[[response_col]] == category,]
categorymap
# extract pixel information
dataSet = raster::extract(raster, categorymap)
dataSet
??raster
?extract
dataSet = raster::extract(raster, categorymap, cellnumbers = T)
dataSet
?coordinates
coordinates(s1vv_re)
names(coordinates(s1vv_re))
dataSet
dataSet[["2"]]
out = list()
dataSet[[2]]
dataSet[[1]]
coordinates(dataSet[[1]]$cell)
dataSet[[1]]
coordinates(as.data.frame(dataSet[[1]]$cell))
new = as.data.frame(dataSet[[1]]$cell)
new = as.data.frame(dataSet[[1]])
new
class(dataSet[[1]])
class(new)
new$cell
coordinates(new$cell)
new = raster(dataSet[[1]])
coordinates(new$cell)
coordinates(new)
new = raster(dataSet)
new = raster(dataSet[[1]])
coordinates(new)
coord_col = coordinates(new)
coord_col = as.data.frame(coordinates(new))
cbind.data.frame(dataSet, coord_col)
cbind.data.frame(dataSet[[1]], coord_col)
coordinatess1vv_re
coordinates(s1vv_re)
source("02_cross_val.R")
dim(s1vh)
1908*2658*353
ncol(s1vh)
xlim(s1vh)
?raster
s1vh
# saveRDS(model, paste0(rds_path, "model_VH"))
model = readRDS(paste0(rds_path, "model_VH"))
model
model$factor.levelsmodel
# pred.task = makeTask
parallelStart(mode = "socket", level = "mlr.resample", cpus = 5)
prediction = predict(model, newdata = as.data.frame(s1vh, xy = TRUE))
parallelStop()
prediction = predict(model, newdata = s1vh)
raster
s1vh
?extent()
raster::extent()
version
package_version("raster")
packageVersion("raster")
bb = extent(521489, 6766136, 542526, 6786709)
bb
raster_small = setExtent(s1vh, bb, keepres = TRUE)
raster_small = setExtent(s1vh, bb)
raster_small
plot(raster_small)
install.packages("rasterViz")
install.packages("rasterVis")
?rasterViz
??rasterVis
rasterVis-package
library(rasterVis)
levelplot(raster_small)
levelplot(raster_small, data = NULL, 1)
levelplot(raster_small[[1]], data = NULL, 1)
levelplot(raster_small[[1]])
levelplot(raster_small[[1:2]])
levelplot(raster_small[[1:4]])
prediction = predict(model, newdata = as.data.frame(raster_small, xy = TRUE))
raster_small
raster_small = setExtent(s1vh, bb)
raster_small
raster_small = crop(s1vh, bb)
levelplot(raster_small[[1:4]])
writeRaster(raster_small, filename = "D:\\Geodaten\\#Jupiter\\GEO402\\01_data\\s1_data\\raster_small",  format="GTiff",
overwrite=TRUE, na.rm=TRUE)
levelplot(raster_small[[1:4]])
raster_small = crop(s1vh, bb, filename = "D:\\Geodaten\\#Jupiter\\GEO402\\01_data\\s1_data\\raster_small",
format="GTiff", overwrite=TRUE, na.rm=TRUE)
raster_small
s1vh_small = brick("D:\\Geodaten\\#Jupiter\\GEO402\\01_data\\s1_data\\raster_small.tif")
s1vh_small = brick("D:\\Geodaten\\#Jupiter\\GEO402\\01_data\\s1_data\\raster_small_vh.tif")
levelplot(raster_small[[1:4]])
levelplot(s1vh_small[[1:4]])
s1vh_small
s1vv
s1vh_small
s1vv
names(s1vh)
dates = names(s1vh)
names(s1vh_small) = dates
names(s1vh_small)
tail(dates)
head(s1vh)
head(dates)
n(dates)
length(dates)
s1vv = brick(s1vv_path)
s1vh = brick(s1vh_path)
s1vh_small = brick("D:\\Geodaten\\#Jupiter\\GEO402\\01_data\\s1_data\\raster_small_vh.tif")
names(s1vh_small) = names(s1vh)
# Revome invalid raster (covering less than half of the area)-------------------
s1vv = rename_bandnames(raster = s1vv) %>%
.[[c(-14, -17, -62)]]
s1vh = rename_bandnames(raster = s1vh) %>%
.[[c(-14, -17, -62)]]
s1vh_small = rename_bandnames(raster = s1vh_small) %>%
.[[c(-14, -17, -62)]]
s1vh_small
s1vh_small = rename_bandnames(raster = s1vh_small) %>%
.[[c(-14, -17, -62)]]
s1vh_small
s1vv = brick(s1vv_path)
s1vh = brick(s1vh_path)
s1vh_small = brick("D:\\Geodaten\\#Jupiter\\GEO402\\01_data\\s1_data\\raster_small_vh.tif")
names(s1vh_small) = names(s1vh) # pre-rename because dataset was created in QGIS with automatic naming
# Revome invalid raster (covering less than half of the area)-------------------
s1vv = rename_bandnames(raster = s1vv) %>%
.[[c(-14, -17, -62)]] # remove brocken files
s1vh = rename_bandnames(raster = s1vh) %>%
.[[c(-14, -17, -62)]]
s1vh_small = rename_bandnames(raster = s1vh_small) %>%
.[[c(-14, -17, -62)]]
gt_path = "D:\\Geodaten\\#Jupiter\\GEO402\\02_features\\ROI_updated.kml"
gt = st_read(gt_path, quiet = TRUE) %>%  # read in
st_transform(st_crs(s1vv)) %>%  # set crs(gt) to the crs(s1) brick.
st_zm(drop = TRUE)  # Remove Z-Dimension
s1vh_small
as.data.frame(s1vh_small, xy = TRUE)
as.data.frame(s1vh_small[[1:5]], xy = TRUE)
as.data.frame(s1vh_small[[1:10]], xy = TRUE)
system.time(as.data.frame(s1vh_small[[1]], xy = TRUE))
system.time(raster_input = as.data.frame(s1vh_small, xy = TRUE)9
system.time(raster_input = as.data.frame(s1vh_small, xy = TRUE))
?system.time
raster_input = as.data.frame(s1vh_small, xy = TRUE)
head(raster_input)
str(raster_input)
View(raster_input)
raster_input[1,1]
raster_input[1,3]
raster_input[,3]
raster_input[,4]
raster_input[,4]+raster_input[,3]
raster_input[,4]*raster_input[,3]
View(raster_input)
saveRDS(raster_input, file = paste0(rds_path, "rf_input.rds"))
raster_input
dim(raster_input)
# pred.task = makeTask
parallelStart(mode = "socket", cpus = 7)
prediction = predict(model, newdata = raster_input)
parallelStop()
prediction
pred = as.data.frame(prediction)
pred
matrix = calculateConfusionMatrix(prediction)
matrix
plotLearnerPrediction(classif.lrn.optimised, task = classif.task)
names(raster_input)
pred
names(pred)
result_xy = cbind(pred, raster_input$x, raster_input$y) %>%
as.data.frame() %>%
dplyr::select(x = "raster_input$x", y = "raster_input$y", class = response)
View(result_xy)
# sf for coordnate system
out_sf = st_as_sf(result_xy, coords = c("x", "y"))
st_crs(out_sf) = 32735
out_sf$class = as.numeric(out_sf$class)
out_sf[out_sf$class == 2,1] = 12
out_sf[out_sf$class == 3,1] = 2
str(out_sf)
out_sf$class
as.factor(out_sf$class)
out_sf$class = as.factor(out_sf$class)
# to sp for gridding, functionality is not yet found in sf... st_rasterize may work in `stars`
out_sp = as(out_sf, "Spatial")
gridded(out_sp) = TRUE
class(out_sp)
?writeRaster
outfile = raster(out_sp) %>%
trim()
writeRaster(outfile, filename = paste0(prediction_out_path, "0121_prediction_rf_crop"),
format="GTiff", datatype='INT1U', overwrite=TRUE, na.rm=TRUE)
prediction_out_path = "D:\\Geodaten\\#Jupiter\\GEO402\\04_products\\rf\\"
writeRaster(outfile, filename = paste0(prediction_out_path, "0121_prediction_rf_crop"),
format="GTiff", datatype='INT1U', overwrite=TRUE, na.rm=TRUE)
prediction
model$subset # no subset
model$learner.model
model$task.desc
c("1", "12", "2", "3", "4")
as.factors(c("1", "12", "2", "3", "4"))
as.factor(c("1", "12", "2", "3", "4"))
classif.lrn.optimised$par.set # Learner
classif.lrn.optimised # Task
classif.lrn.optimised$par.set # Learner
classif.task
# Input of stack, which is containing training and reference data
if (!file.exists(paste0(rds_path, "learning_input_VH.rds"))) {
print("file does not exist")
gt_from_raster(raster = s1vh, outfile = "VH")
data_input = readRDS(paste0(rds_path, "learning_input_VH.rds"))
} else {data_input = readRDS(paste0(rds_path, "learning_input_VH.rds"))}
data_input
data_input$X2015.03.11
data_input$class
str(data_input$class)
data_input$class
map(data_input$class, mean)
map(data_input$class)
data_input$class %>%
group_by(class) %>%
summarise(n = n())
View(data_input$class)
View(gt_from_raster)
# performance
slangbos_spcv
slangbos_spcv$aggr
slangbos_spcv$measures.test
matrix
matrix = calculateConfusionMatrix(prediction)
n = getTaskSize(classif.task)
n
training.set = sample(n, size = n/3)
prediction
source("02_cross_val.R")
classif.task = makeClassifTask(
id = "slangbos", data = s1vh, target = "class",
coordinates = coords
)
