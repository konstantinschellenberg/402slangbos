dataSet = raster::extract(raster, categorymap)
out = list()
for (a in 1:length(dataset)){
print("inner", a)
t = as.data.frame(dataset[[a]]) %>%
t() %>%
as.data.frame() %>%
mutate(date = colnames(dataset[[1]])) %>%
pivot_longer(-date, names_to = "names", values_to = "values") %>%
group_by(date) %>%
summarise(mean = mean(values), # here can be put more stats information retrieved from the polygons
median = median(values),
sd = sd(values),
"lower_sd" = mean(values) - sd(values),
"upper_sd" = mean(values) + sd(values),
count = n())
out = append(out, list(as.data.frame(t)))
}
# making dataset for machine learning-----------------------------------
dataSet = dataSet[!unlist(lapply(dataSet, is.null))]
dataSet = lapply(dataSet, function(x){cbind(x, class = as.numeric(rep(category, nrow(x))))})
df = do.call("rbind", dataSet)
df_all = rbind(df_all, df)
# ----------------------------------------------------------------------
# rename inner lists (of the categories)
names(out) = c(rep(category, length(dataset)))
# append to master-list (outest)
outest = append(outest, list(out))
}
saveRDS(df_all, paste0(rds_path, "learning_input.rds"))
saveRDS(outest, paste0(rds_path, "gt_list.rds"))
}
raster = s1vv
df_all = data.frame(matrix(vector(), nrow = 0, ncol = length(names(raster)) + 1))
outest = list()
# get class
category = unique(train_data[[response_col]])[i]
train_data = gt
source("import.R")
train_data = gt
repsonse_col = "Name"
raster = s1vh
df_all = data.frame(matrix(vector(), nrow = 0, ncol = length(names(raster)) + 1))
outest = list()
1:length(unique(train_data[[response_col]])
)
response_col
repsonse_col = "Name"
response_col
response_col = "Name"
response_col
train_data[[response_col]]
category = unique(train_data[[response_col]])[i]
print(category)
# get class
category = unique(train_data[[response_col]])[i]
i = 1
# get class
category = unique(train_data[[response_col]])[i]
print(category)
i = 2
# get class
category = unique(train_data[[response_col]])[i]
print(category)
# returns sp polygon with class i
categorymap = train_data[train_data[[response_col]] == category,]
categorymap
length(categorymap)
ncol(categorymap)
nrow(categorymap)
# rename inner lists (of the categories)
names(out) = c(seq(1:nrow(categorymap)))
out = list()
# rename inner lists (of the categories)
names(out) = c(seq(1:nrow(categorymap)))
c(seq(1:nrow(categorymap)))
train_data[[1]]
(unique(train_data[[response_col]])
)
c(unique(train_data[[response_col]]))
unique(train_data[[response_col]])
gt_from_raster = function(train_data = gt,
response_col = "Name",
raster = s1vh){
# credits to http://amsantac.co/blog/en/2015/11/28/classification-r.html
# https://gist.github.com/amsantac/5183c0c71a8dcbc27a4f
df_all = data.frame(matrix(vector(), nrow = 0, ncol = length(names(raster)) + 1))
outest = list()
for (i in 1:length(unique(train_data[[response_col]]))){
# get class
category = unique(train_data[[response_col]])[i]
print(category)
# returns sp polygon with class i
categorymap = train_data[train_data[[response_col]] == category,]
# extract pixel information
dataSet = raster::extract(raster, categorymap)
out = list()
for (a in 1:length(dataset)){
print("inner", a)
t = as.data.frame(dataset[[a]]) %>%
t() %>%
as.data.frame() %>%
mutate(date = colnames(dataset[[1]])) %>%
pivot_longer(-date, names_to = "names", values_to = "values") %>%
group_by(date) %>%
summarise(mean = mean(values), # here can be put more stats information retrieved from the polygons
median = median(values),
sd = sd(values),
"lower_sd" = mean(values) - sd(values),
"upper_sd" = mean(values) + sd(values),
count = n())
out = append(out, list(as.data.frame(t)))
}
# making dataset for machine learning-----------------------------------
dataSet = dataSet[!unlist(lapply(dataSet, is.null))]
dataSet = lapply(dataSet, function(x){cbind(x, class = as.numeric(rep(category, nrow(x))))})
df = do.call("rbind", dataSet)
df_all = rbind(df_all, df)
# ----------------------------------------------------------------------
# rename inner lists (of the categories)
names(out) = c(seq(1:nrow(categorymap)))
# append to master-list (outest)
outest = append(outest, list(out))
}
names(outest) = unique(train_data[[response_col]])
saveRDS(df_all, paste0(rds_path, "learning_input.rds"))
saveRDS(outest, paste0(rds_path, "gt_list.rds"))
}
# credits to http://amsantac.co/blog/en/2015/11/28/classification-r.html
# https://gist.github.com/amsantac/5183c0c71a8dcbc27a4f
df_all = data.frame(matrix(vector(), nrow = 0, ncol = length(names(raster)) + 1))
outest = list()
for (i in 1:length(unique(train_data[[response_col]]))){
# get class
category = unique(train_data[[response_col]])[i]
print(category)
# returns sp polygon with class i
categorymap = train_data[train_data[[response_col]] == category,]
# extract pixel information
dataSet = raster::extract(raster, categorymap)
out = list()
for (a in 1:length(dataset)){
print("inner", a)
t = as.data.frame(dataset[[a]]) %>%
t() %>%
as.data.frame() %>%
mutate(date = colnames(dataset[[1]])) %>%
pivot_longer(-date, names_to = "names", values_to = "values") %>%
group_by(date) %>%
summarise(mean = mean(values), # here can be put more stats information retrieved from the polygons
median = median(values),
sd = sd(values),
"lower_sd" = mean(values) - sd(values),
"upper_sd" = mean(values) + sd(values),
count = n())
out = append(out, list(as.data.frame(t)))
}
# making dataset for machine learning-----------------------------------
dataSet = dataSet[!unlist(lapply(dataSet, is.null))]
dataSet = lapply(dataSet, function(x){cbind(x, class = as.numeric(rep(category, nrow(x))))})
df = do.call("rbind", dataSet)
df_all = rbind(df_all, df)
# ----------------------------------------------------------------------
# rename inner lists (of the categories)
names(out) = c(seq(1:nrow(categorymap)))
# append to master-list (outest)
outest = append(outest, list(out))
}
f
gt_from_raster = function(train_data = gt,
response_col = "Name",
raster = s1vh,
outfile = "s1vh"){
# credits to http://amsantac.co/blog/en/2015/11/28/classification-r.html
# https://gist.github.com/amsantac/5183c0c71a8dcbc27a4f
df_all = data.frame(matrix(vector(), nrow = 0, ncol = length(names(raster)) + 1))
outest = list()
for (i in 1:length(unique(train_data[[response_col]]))){
# get class
category = unique(train_data[[response_col]])[i]
print(category)
# returns sp polygon with class i
categorymap = train_data[train_data[[response_col]] == category,]
# extract pixel information
dataSet = raster::extract(raster, categorymap)
out = list()
for (a in 1:length(dataset)){
print("inner", a)
t = as.data.frame(dataset[[a]]) %>%
t() %>%
as.data.frame() %>%
mutate(date = colnames(dataset[[1]])) %>%
pivot_longer(-date, names_to = "names", values_to = "values") %>%
group_by(date) %>%
summarise(mean = mean(values), # here can be put more stats information retrieved from the polygons
median = median(values),
sd = sd(values),
"lower_sd" = mean(values) - sd(values),
"upper_sd" = mean(values) + sd(values),
count = n())
out = append(out, list(as.data.frame(t)))
}
# making dataset for machine learning-----------------------------------
dataSet = dataSet[!unlist(lapply(dataSet, is.null))]
dataSet = lapply(dataSet, function(x){cbind(x, class = as.numeric(rep(category, nrow(x))))})
df = do.call("rbind", dataSet)
df_all = rbind(df_all, df)
# ----------------------------------------------------------------------
# rename inner lists (of the categories)
names(out) = c(seq(1:nrow(categorymap)))
# append to master-list (outest)
outest = append(outest, list(out))
}
names(outest) = unique(train_data[[response_col]])
saveRDS(df_all, paste0(rds_path, "learning_input", outfile, ".rds"))
saveRDS(outest, paste0(rds_path, "gt_list", outfile, ".rds"))
}
s1vv = s1vv[1:10]
names(s1vv)
s1vv = s1vv[[1]]
plot(s1vv)
source("import.R")
s1vv = s1vv[[1:10]]
source("import.R")
s1vv_re = s1vv[[1:10]]
View(s1vv_re)
1:10
seq(1,10
)
s1vv[[c(seq(1,10))]]
s1vv_re = s1vv[[c(seq(1,10))]]
s1vv_re
names(s1vv_re)
[[seq(1, 10)]]
seq(1, 10)
list_rasters = list(s1vv, s1vh)
list_rasters
View(list_rasters)
for i in list_rasters{
i = i[[seq(1, 3)]]
gt_from_raster(train_data = gt, response_col = "Name", raster = i, outfile = chr(i))
}
gt_from_raster(raster = s1vv_re, ooutfile = "test")
gt_from_raster(raster = s1vv_re, outfile = "test")
View(s1vv_re)
s1vv_re = s1vv[[seq(1,3)]]
View(s1vv_re)
View(s1vv)
View(s1vv_re)
gt_from_raster(raster = s1vv_re, outfile = "test")
gt_from_raster = function(train_data = gt,
response_col = "Name",
raster = s1vh,
outfile = "s1vh"){
# credits to http://amsantac.co/blog/en/2015/11/28/classification-r.html
# https://gist.github.com/amsantac/5183c0c71a8dcbc27a4f
df_all = data.frame(matrix(vector(), nrow = 0, ncol = length(names(raster)) + 1))
outest = list()
for (i in 1:length(unique(train_data[[response_col]]))){
# get class
category = unique(train_data[[response_col]])[i]
print(category)
# returns sp polygon with class i
categorymap = train_data[train_data[[response_col]] == category,]
# extract pixel information
dataSet = raster::extract(raster, categorymap)
out = list()
for (a in 1:length(dataSet)){
print("inner", a)
t = as.data.frame(dataSet[[a]]) %>%
t() %>%
as.data.frame() %>%
mutate(date = colnames(dataSet[[1]])) %>%
pivot_longer(-date, names_to = "names", values_to = "values") %>%
group_by(date) %>%
summarise(mean = mean(values), # here can be put more stats information retrieved from the polygons
median = median(values),
sd = sd(values),
"lower_sd" = mean(values) - sd(values),
"upper_sd" = mean(values) + sd(values),
count = n())
out = append(out, list(as.data.frame(t)))
}
# making dataset for machine learning-----------------------------------
dataSet = dataSet[!unlist(lapply(dataSet, is.null))]
dataSet = lapply(dataSet, function(x){cbind(x, class = as.numeric(rep(category, nrow(x))))})
df = do.call("rbind", dataSet)
df_all = rbind(df_all, df)
# ----------------------------------------------------------------------
# rename inner lists (of the categories)
names(out) = c(seq(1:nrow(categorymap)))
# append to master-list (outest)
outest = append(outest, list(out))
}
names(outest) = unique(train_data[[response_col]])
saveRDS(df_all, paste0(rds_path, "learning_input", outfile, ".rds"))
saveRDS(outest, paste0(rds_path, "gt_list", outfile, ".rds"))
}
gt_from_raster = function(train_data = gt,
response_col = "Name",
raster = s1vh,
outfile = "s1vh"){
# credits to http://amsantac.co/blog/en/2015/11/28/classification-r.html
# https://gist.github.com/amsantac/5183c0c71a8dcbc27a4f
df_all = data.frame(matrix(vector(), nrow = 0, ncol = length(names(raster)) + 1))
outest = list()
for (i in 1:length(unique(train_data[[response_col]]))){
# get class
category = unique(train_data[[response_col]])[i]
print(category)
# returns sp polygon with class i
categorymap = train_data[train_data[[response_col]] == category,]
# extract pixel information
dataSet = raster::extract(raster, categorymap)
out = list()
for (a in 1:length(dataSet)){
print("inner", a)
t = as.data.frame(dataSet[[a]]) %>%
t() %>%
as.data.frame() %>%
mutate(date = colnames(dataSet[[1]])) %>%
pivot_longer(-date, names_to = "names", values_to = "values") %>%
group_by(date) %>%
summarise(mean = mean(values), # here can be put more stats information retrieved from the polygons
median = median(values),
sd = sd(values),
"lower_sd" = mean(values) - sd(values),
"upper_sd" = mean(values) + sd(values),
count = n())
out = append(out, list(as.data.frame(t)))
}
# making dataset for machine learning-----------------------------------
dataSet = dataSet[!unlist(lapply(dataSet, is.null))]
dataSet = lapply(dataSet, function(x){cbind(x, class = as.numeric(rep(category, nrow(x))))})
df = do.call("rbind", dataSet)
df_all = rbind(df_all, df)
# ----------------------------------------------------------------------
# rename inner lists (of the categories)
names(out) = c(seq(1:nrow(categorymap)))
# append to master-list (outest)
outest = append(outest, list(out))
}
names(outest) = unique(train_data[[response_col]])
saveRDS(df_all, paste0(rds_path, "learning_input", outfile, ".rds"))
saveRDS(outest, paste0(rds_path, "gt_list", outfile, ".rds"))
}
s1vv_re = s1vv[[seq(1,3)]]
gt_from_raster(raster = s1vv_re, outfile = "test")
readRDS(paste0(rds_path, "learning_input", test, ".rds")
readRDS(paste0(rds_path, "learning_input", test, ".rds"))
rds_path
readRDS(paste0(rds_path, "learning_inputtest.rds"))
leaning_input = readRDS(paste0(rds_path, "learning_inputtest.rds"))
leaning_input
gt_list = readRDS(paste0(rds_path, "gt_listtest.rds"))
gt_list
gt_list$1
gt_list$12
gt_list$"12"
system.time(gt_from_raster(raster = s1vh, outfile = "_test"))
######################################################################
# Data Preparation
######################################################################
rds_path
library(mlr)
library(data.table)
library(parallelMap)
library(raster)
# subset for test data
s1vv_re = s1vv[[seq(1,3)]]
# time consuming task!
system.time(gt_from_raster(raster = s1vv_re, outfile = "_test"))
learning_input = readRDS(paste0(rds_path, "learning_input_VH.rds"))
learning_input
names(learning_input)
learning_input_test = readRDS(paste0(rds_path, "learning_input_test.rds"))
View(learning_input_test)
s1vv_re = i
chr(i)
toString(i)
i
i = s1vv_re
toString(i)
for i in list_rasters{
i = i[[seq(1, 3)]]
library(mlr)
library(data.table)
library(parallelMap)
library(raster)
source("import.R")
# Input of stack, which is containing training and reference data
data_input = readRDS(paste0(rds_path, "learning_input_test.rds"))
View(data_input)
data.table::data.table(data_input)
data_input
# Remove NoData values
data <- na.omit(data_input)
# Input of stack, which is containing training and reference data
data_input = readRDS(paste0(rds_path, "learning_input_test.rds"))
data_input = na.omit(data_input)
classif.task <- makeClassifTask(
id = "slangbos", data = data, target = "class"
)
class(data_input$class)
as_factor(data_input$class)
as.factor(data_input$class)
data_input$class = as_factor(data_input$class)
data_input$class
class(data_input$class)
classif.task <- makeClassifTask(
id = "slangbos", data = data, target = "class"
)
data_input$class = as_factor(data_input$class) # assign class as factor, not numeric
classif.task <- makeClassifTask(
id = "slangbos", data = data, target = "class"
)
classif.task <- makeClassifTask(
id = "slangbos", data = data_input, target = "class"
)
classif.task
# Create Learner
# Classification tree, set it up for predicting probabilities
classif.lrn = makeLearner("classif.randomForest", predict.type = "response", fix.factors.prediction = TRUE)
classif.lrn
# show default parameters
getParamSet(classif.lrn)
# show which parameters are tunable
filterParams(getParamSet(classif.lrn), tunable = TRUE)
mlr
vignette("mlr")
# Defining parameter set
ps <- makeParamSet(
makeIntegerParam("mtry", lower = 1, upper = 4),
makeDiscreteParam("num.trees", values = c(10,50,100,300,700))
# for random selection of num.trees use: makeIntegerParam("num.trees", lower = 10, upper = 700)
)
ps
# Define the inner reampling iterations
ctrl <- makeTuneControlGrid() # for random selection of parameters use: ctrl <- makeTuneControlRandom(maxit = 100)
makeResampleDesc()
?makeResampleDesc()
inner <- makeResampleDesc("SpCV", iters = 5)
inner
parallelStart(mode = "socket", level = "mlr.tuneParams", cpus = 5)
# Tuning the Random Forest
tune_rf <- tuneParams(classif.lrn,
task = classif.task, resampling = inner, par.set = ps,
control = ctrl, show.info = TRUE, measures = setAggregation(rmse, test.mean)
)
?Task
s1vv_re
# subset for test data
s1vv_re = s1vv[[seq(1,3)]]
# subset for test data
raster_test = s1vv[[seq(1,3)]]
?raster::extrace
?raster::extract
# Create data frame
data <- as.data.frame(data_input, xy = TRUE)
data
as.data.frame()
?as.data.frame()
?as.data.frame
train_data = gt
response_col = "Name"
raster = s1vh
raster = s1vv_re
df_all = data.frame(matrix(vector(), nrow = 0, ncol = length(names(raster)) + 1))
outest = list()
# get class
category = unique(train_data[[response_col]])[i]
category
# returns sp polygon with class i
categorymap = train_data[train_data[[response_col]] == category,]
categorymap
# extract pixel information
dataSet = raster::extract(raster, categorymap)
dataSet
??raster
?extract
dataSet = raster::extract(raster, categorymap, cellnumbers = T)
dataSet
?coordinates
coordinates(s1vv_re)
names(coordinates(s1vv_re))
dataSet
dataSet[["2"]]
out = list()
dataSet[[2]]
dataSet[[1]]
coordinates(dataSet[[1]]$cell)
dataSet[[1]]
coordinates(as.data.frame(dataSet[[1]]$cell))
new = as.data.frame(dataSet[[1]]$cell)
new = as.data.frame(dataSet[[1]])
new
class(dataSet[[1]])
class(new)
new$cell
coordinates(new$cell)
new = raster(dataSet[[1]])
coordinates(new$cell)
coordinates(new)
new = raster(dataSet)
new = raster(dataSet[[1]])
coordinates(new)
coord_col = coordinates(new)
coord_col = as.data.frame(coordinates(new))
cbind.data.frame(dataSet, coord_col)
cbind.data.frame(dataSet[[1]], coord_col)
coordinatess1vv_re
coordinates(s1vv_re)
